{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import copy\n",
    "import os\n",
    "from bisect import bisect_left\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import enum\n",
    "import tensorflow as tf\n",
    "\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.loss import CrossEntropy\n",
    "from cleverhans.dataset import MNIST\n",
    "from cleverhans.model import Model\n",
    "from cleverhans.picklable_model import MLP, Conv2D, ReLU, Flatten, Linear, Softmax\n",
    "from cleverhans.train import train\n",
    "from cleverhans.utils_tf import batch_eval, model_eval\n",
    "from cleverhans import serial\n",
    "from pathlib import Path\n",
    "\n",
    "from dknn import DkNNModel, get_tensorflow_session, make_basic_picklable_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = sys.argv[:1]\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_integer(\n",
    "    'number_bits',\n",
    "    17,\n",
    "    'number of hash bits used by LSH Index'\n",
    "  )\n",
    "tf.flags.DEFINE_float(\n",
    "    'tensorflow_gpu_memory_fraction',\n",
    "    0.25,\n",
    "    'amount of the GPU memory to allocate for a tensorflow Session'\n",
    "  )\n",
    "tf.flags.DEFINE_integer('nb_epochs', 6, 'Number of epochs to train model')\n",
    "tf.flags.DEFINE_integer('batch_size', 500, 'Size of training batches')\n",
    "tf.flags.DEFINE_float('lr', 0.001, 'Learning rate for training')\n",
    "\n",
    "tf.flags.DEFINE_integer(\n",
    "      'nb_cali', 750, 'Number of calibration points for the DkNN')\n",
    "tf.flags.DEFINE_integer(\n",
    "      'neighbors', 75, 'Number of neighbors per layer for the DkNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST(train_start=0, train_end=10000, test_start=0, test_end=1000)\n",
    "x_train, y_train = mnist.get_set('train')\n",
    "x_test, y_test = mnist.get_set('test')\n",
    "\n",
    "# Use Image Parameters.\n",
    "img_rows, img_cols, nchannels = x_train.shape[1:4]\n",
    "nb_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_tensorflow_session() as sess:\n",
    "    with tf.variable_scope('dknn'):\n",
    "        # Define input TF placeholder.\n",
    "        x = tf.placeholder(tf.float32, shape=(\n",
    "          None, img_rows, img_cols, nchannels))\n",
    "        y = tf.placeholder(tf.float32, shape=(None, nb_classes))\n",
    "\n",
    "        # Define a model.\n",
    "        model = make_basic_picklable_cnn()\n",
    "        preds = model.get_logits(x)\n",
    "        loss = CrossEntropy(model, smoothing=0.)\n",
    "\n",
    "        # Define the test set accuracy evaluation.\n",
    "        def evaluate():\n",
    "            acc = model_eval(sess, x, y, preds, x_test, y_test,\n",
    "                             args={'batch_size': FLAGS.batch_size})\n",
    "            print('Test accuracy on test examples: %0.4f' % acc)\n",
    "\n",
    "        # Train the model\n",
    "        train_params = {'nb_epochs': FLAGS.nb_epochs,\n",
    "                      'batch_size': FLAGS.batch_size, 'learning_rate': FLAGS.lr}\n",
    "\n",
    "        data_filepath = \"model.joblib\"\n",
    "        path = Path(data_filepath)\n",
    "\n",
    "        if path.is_file():\n",
    "            model = serial.load(data_filepath)\n",
    "        else:\n",
    "            train(sess, loss, x_train, y_train, evaluate=evaluate,\n",
    "              args=train_params, var_list=model.get_params())\n",
    "            serial.save(\"model.joblib\", model)\n",
    "\n",
    "          # Define callable that returns a dictionary of all activations for a dataset\n",
    "        def get_activations(data):\n",
    "            data_activations = {}\n",
    "            for layer in layers:\n",
    "                layer_sym = tf.layers.flatten(model.get_layer(x, layer))\n",
    "                data_activations[layer] = batch_eval(sess, [x], [layer_sym], [data],\n",
    "                                                   args={'batch_size': FLAGS.batch_size})[0]\n",
    "            return data_activations\n",
    "\n",
    "        # Use a holdout of the test set to simulate calibration data for the DkNN.\n",
    "        train_data = x_train\n",
    "        train_labels = np.argmax(y_train, axis=1)\n",
    "        cali_data = x_test[:FLAGS.nb_cali]\n",
    "        y_cali = y_test[:FLAGS.nb_cali]\n",
    "        cali_labels = np.argmax(y_cali, axis=1)\n",
    "        test_data = x_test[FLAGS.nb_cali:]\n",
    "        y_test = y_test[FLAGS.nb_cali:]\n",
    "\n",
    "        # Extract representations for the training and calibration data at each layer of interest to the DkNN.\n",
    "        layers = ['ReLU1', 'ReLU3', 'ReLU5', 'logits']\n",
    "\n",
    "        # Wrap the model into a DkNNModel\n",
    "        dknn = DkNNModel(\n",
    "        FLAGS.neighbors,\n",
    "        layers,\n",
    "        get_activations,\n",
    "        train_data,\n",
    "        train_labels,\n",
    "        nb_classes,\n",
    "        scope='dknn'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_kernel import euclidean_kernel, hard_geodesics_euclidean_kernel\n",
    "from utils_visualization import plot_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hola_x = x_train[0:10000].reshape((10000, 28*28))\n",
    "hola_y = train_labels[0:10000]\n",
    "hola_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_matrix = euclidean_kernel(hola_x)\n",
    "max_distance = np.max(euclidean_matrix)+1\n",
    "euclidean_matrix[euclidean_matrix == 0]=max_distance\n",
    "plot_kernel(euclidean_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodesic_euclidean_matrix = hard_geodesics_euclidean_kernel(hola_x, 5)\n",
    "max_distance = np.max(geodesic_euclidean_matrix)+1\n",
    "geodesic_euclidean_matrix[geodesic_euclidean_matrix == 0]=max_distance\n",
    "plot_kernel(geodesic_euclidean_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1,20)\n",
    "same_class_euclidean = np.zeros(len(ks))\n",
    "same_class_geodesic = np.zeros(len(ks))\n",
    "for j,k in enumerate(ks):\n",
    "    acum_euc = 0\n",
    "    acum_geo = 0\n",
    "    for i in range(10000):\n",
    "        euclidean_neighbors_idx = np.argpartition(euclidean_matrix[i,:],k)[:k]\n",
    "        acum_euc += np.mean(hola_y[i]==hola_y[euclidean_neighbors_idx])\n",
    "\n",
    "        geodesic_neighbors_idx = np.argpartition(geodesic_euclidean_matrix[i,:],k)[:k]\n",
    "        acum_geo += np.mean(hola_y[i]==hola_y[geodesic_neighbors_idx])\n",
    "    same_class_euclidean[j] = acum_euc/10000\n",
    "    same_class_geodesic[j] = acum_geo/10000\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,20),same_class_euclidean)\n",
    "plt.plot(range(1,20),same_class_geodesic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = '../results/geodesic_matrices_1000_5.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(matrix_path, 'rb') as f:\n",
    "    hola = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
